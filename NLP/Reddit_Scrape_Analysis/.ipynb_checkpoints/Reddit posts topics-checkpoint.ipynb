{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Topics from Posts in the r/Python subreddit\n",
    "\n",
    "The aim of this project is to classify the topic of posts from the r/Python subreddit from end-to-end. In order to do this, I first needed to scrape a sufficient number of posts for an accurate NLP model. I have done this via this [submission downloader code](https://github.com/dgadish/projects/blob/master/NLP/Reddit_Scrape_Analysis/Reddit%20Submission%20downloader.ipynb) . \n",
    "\n",
    "Once a sufficient number of submissions were scraped, I read them into a pandas dataframe in order to clean them sufficiently to be fed into a model.\n",
    "\n",
    "The submissions were collected at around 10:00 on 07/02/2021\n",
    "\n",
    "The submissions have both title and text sections. I will attempt to determine topics using both. \n",
    "\n",
    "I will attempt to use both LSA (Latent Semantic Analysis) and LDA (Latent Dirichlet Allocation) to determine the topics of each post. When I have more time, I will attempt to use Word Embedding combined with something like K-Means clustering as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress so far...\n",
    "\n",
    "So far I have imported the submissions and I have cleaned and preprocessed the text sections of the submissions. I have then created a bag-of-words model for the text column and used it to train an LDA model.\n",
    "\n",
    "### Next steps...\n",
    "\n",
    "Next I need to play around with the parameters of the model to find suitable topics with which I can label and then assign to submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all')\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.read_json('python_posts.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python conversion tool for converting csv to J...</td>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1612467290</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lynda courses</td>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1612466655</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anybody have sample code to determine if a web...</td>\n",
       "      <td>Python</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1612466550</td>\n",
       "      <td>Hi, has anyone created python code that tests ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barnsley Fern - an interesting fractal created...</td>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1612465003</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Project – Find Neapolitan pizza with AI help</td>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1612464251</td>\n",
       "      <td>Just finished this project! An unfiltered revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title subreddit  score  \\\n",
       "1  Python conversion tool for converting csv to J...    Python      1   \n",
       "2                                      lynda courses    Python      1   \n",
       "3  Anybody have sample code to determine if a web...    Python      4   \n",
       "5  Barnsley Fern - an interesting fractal created...    Python      1   \n",
       "6       Project – Find Neapolitan pizza with AI help    Python      2   \n",
       "\n",
       "   num_comments  created_utc  \\\n",
       "1             1   1612467290   \n",
       "2             2   1612466655   \n",
       "3             5   1612466550   \n",
       "5             0   1612465003   \n",
       "6             0   1612464251   \n",
       "\n",
       "                                            selftext  \n",
       "1                                          [removed]  \n",
       "2                                          [removed]  \n",
       "3  Hi, has anyone created python code that tests ...  \n",
       "5                                          [deleted]  \n",
       "6  Just finished this project! An unfiltered revi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26003 entries, 1 to 40100\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         26003 non-null  object\n",
      " 1   subreddit     26003 non-null  object\n",
      " 2   score         26003 non-null  int64 \n",
      " 3   num_comments  26003 non-null  int64 \n",
      " 4   created_utc   26003 non-null  int64 \n",
      " 5   selftext      26003 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "subs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop submissions which have been removed or deleted\n",
    "\n",
    "n_r_d = (subs['selftext'] != '[removed]') & (subs['selftext'] != '[deleted]')\n",
    "subs_1 = subs[n_r_d]\n",
    "subs_1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anybody have sample code to determine if a web...</td>\n",
       "      <td>Python</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1612466550</td>\n",
       "      <td>Hi, has anyone created python code that tests ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Project – Find Neapolitan pizza with AI help</td>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1612464251</td>\n",
       "      <td>Just finished this project! An unfiltered revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In response to the \"Medium bad\" thread, here a...</td>\n",
       "      <td>Python</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1612460552</td>\n",
       "      <td>I agree with some of the sentiments shared in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To open source or not to open source?</td>\n",
       "      <td>Python</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1612453887</td>\n",
       "      <td>When do you guys know when to open source a pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can i decrypt signature Url of YouTube Videos</td>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1612453834</td>\n",
       "      <td>I made a python module which download youtube ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title subreddit  score  \\\n",
       "0  Anybody have sample code to determine if a web...    Python      4   \n",
       "1       Project – Find Neapolitan pizza with AI help    Python      2   \n",
       "2  In response to the \"Medium bad\" thread, here a...    Python      9   \n",
       "3              To open source or not to open source?    Python      3   \n",
       "4  How can i decrypt signature Url of YouTube Videos    Python      2   \n",
       "\n",
       "   num_comments  created_utc  \\\n",
       "0             5   1612466550   \n",
       "1             0   1612464251   \n",
       "2             4   1612460552   \n",
       "3             8   1612453887   \n",
       "4             3   1612453834   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi, has anyone created python code that tests ...  \n",
       "1  Just finished this project! An unfiltered revi...  \n",
       "2  I agree with some of the sentiments shared in ...  \n",
       "3  When do you guys know when to open source a pr...  \n",
       "4  I made a python module which download youtube ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18102 entries, 0 to 18101\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         18102 non-null  object\n",
      " 1   subreddit     18102 non-null  object\n",
      " 2   score         18102 non-null  int64 \n",
      " 3   num_comments  18102 non-null  int64 \n",
      " 4   created_utc   18102 non-null  int64 \n",
      " 5   selftext      18102 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 848.7+ KB\n"
     ]
    }
   ],
   "source": [
    "subs_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and pre-processing\n",
    "\n",
    "Now that I have collected a sufficiently large data set and dropped any submissions that were deleted or removed, I can begin to clean the data set. For now my cleaning will be focused on preparing the data for a bag-of-words model, suitable for LSA and LDA\n",
    "\n",
    "I will begin with the 'selftext' column and then move to the 'title' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selftext**\n",
    "\n",
    "I will start by removing the url's which are included in a number of the submissions. I have taken a regex expression from [Github Gist user gruber](https://gist.github.com/gruber/8891611). To remove urls with pandas methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a new copy of the dataframe\n",
    "\n",
    "subs_2 = subs_1.copy()\n",
    "\n",
    "# Remove url's with provided regex\n",
    "\n",
    "rgx = r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\n",
    "subs_2['selftext'] = subs_2['selftext'].str.replace(rgx,\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I shall remove punctuation, tokenize the text and remove any stop words and words less than 4 letters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and convert all to lowercase\n",
    "\n",
    "subs_2['selftext'] = subs_2['selftext'].str.replace('\\W', ' ').str.lower()\n",
    "\n",
    "# Tockenize\n",
    "\n",
    "subs_2['selftext'] = subs_2['selftext'].apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and anything less than 4 letters long\n",
    "''' \n",
    "By the very nature of the subreddit, every submission should be about python.\n",
    "As such, the word 'python' can also be removed.\n",
    "\n",
    "After running the model, the term 'x200b' is common. \n",
    "This is to do with a zero-width space character and is in the dataset due to how the data is encoded. \n",
    "It does not add any information and so will be removed.\n",
    "'''\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('python')\n",
    "stopwords.add('x200b')\n",
    "\n",
    "def no_stop(list):\n",
    "    \n",
    "    nostops = []\n",
    "    \n",
    "    for l in list:\n",
    "        if l not in stopwords:\n",
    "            nostops.append(l)\n",
    "    \n",
    "    return nostops\n",
    "\n",
    "            \n",
    "def no_short(list):\n",
    "    \n",
    "    noshort = []\n",
    "    \n",
    "    for l in list:\n",
    "        if len(l) >= 4:\n",
    "            noshort.append(l)\n",
    "    \n",
    "    return noshort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strin = 'This is a test string to see if my functions work super duper well'\n",
    "strin = nltk.word_tokenize(strin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'test',\n",
       " 'string',\n",
       " 'see',\n",
       " 'functions',\n",
       " 'work',\n",
       " 'super',\n",
       " 'duper',\n",
       " 'well']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop(strin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'test', 'string', 'functions', 'work', 'super', 'duper', 'well']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_short(strin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply my tested functions to the 'selftext' column\n",
    "\n",
    "subs_2['selftext'] = subs_2['selftext'].apply(no_stop).apply(no_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [anyone, created, code, tests, page, different...\n",
       "1    [finished, project, unfiltered, review, pizza,...\n",
       "2    [agree, sentiments, shared, thread, medium, tu...\n",
       "3    [guys, know, open, source, project, working, p...\n",
       "4    [made, module, download, youtube, videos, with...\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_2['selftext'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in processing the data is to lemmetize and stem the words to bring everything into the present tense and remove endings such as 'ed', 'ly', 's' etc.\n",
    "\n",
    "This can be done using proven functions in the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wnl.lemmatize() takes two arguements, the word and a 'part-of-speech' tag. For now I will focus on just converting the verbs, pos='v', but I may come back to this in the future to ammend it to account for all words such that the stemmer, which results in stems wich aren't proper English, isn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize and stem the text\n",
    "\n",
    "def lemmatize_stemmer(list):\n",
    "    new_list = []\n",
    "    for word in list:\n",
    "        new_list.append(stemmer.stem(wnl.lemmatize(word, pos='v')))\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finished', 'project', 'unfiltered', 'review', 'pizza', 'places', 'city', 'authentic', 'pizza', 'places', 'boston', 'said', 'combining', 'computer', 'vision', 'machine', 'learning', 'ease', 'search', 'neapolitan', 'pizza', 'based', 'photos', 'public', 'crowd', 'sourced', 'reviews', 'check', 'city', 'learn', 'going', 'hood', 'would', 'glad', 'receive', 'feedback']\n",
      "['finish', 'project', 'unfilt', 'review', 'pizza', 'place', 'citi', 'authent', 'pizza', 'place', 'boston', 'say', 'combin', 'comput', 'vision', 'machin', 'learn', 'eas', 'search', 'neapolitan', 'pizza', 'base', 'photo', 'public', 'crowd', 'sourc', 'review', 'check', 'citi', 'learn', 'go', 'hood', 'would', 'glad', 'receiv', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "# Test function of some rows in subs_2\n",
    "\n",
    "slftxt = subs_2['selftext'][1]\n",
    "print(slftxt)\n",
    "cleaned = lemmatize_stemmer(slftxt)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not perfect, this will be good enough for the purpose of this project at this stage. The function will now be applied to subs_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_2['selftext'] = subs_2['selftext'].apply(lemmatize_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [anyon, creat, code, test, page, differ, previ...\n",
       "1    [finish, project, unfilt, review, pizza, place...\n",
       "2    [agre, sentiment, share, thread, medium, turn,...\n",
       "3    [guy, know, open, sourc, project, work, projec...\n",
       "4    [make, modul, download, youtub, video, without...\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_2['selftext'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to produce a bag-of-words model from the texts. This can be done with the assistance of the gensim package.\n",
    "\n",
    "First, a dictionary containing a word count for each word will be produced. This will then be cut down by filtering out any words that appear in less than 20 documents and in more than 50 % as these words will be be either to rare or to common to use to classify topics.\n",
    "\n",
    "Once this has been done, the dictionary will be used to create a bag-of-words model for each submission in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slf_dictionary = gensim.corpora.Dictionary(subs_2['selftext'])\n",
    "slf_dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "slf_bow_corpus = [slf_dictionary.doc2bow(doc) for doc in subs_2['selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"anyon\") appears 2 time.\n",
      "Word 1 (\"autom\") appears 1 time.\n",
      "Word 2 (\"basic\") appears 1 time.\n",
      "Word 3 (\"code\") appears 1 time.\n",
      "Word 4 (\"creat\") appears 1 time.\n",
      "Word 5 (\"differ\") appears 1 time.\n",
      "Word 6 (\"display\") appears 1 time.\n",
      "Word 7 (\"enough\") appears 1 time.\n",
      "Word 8 (\"get\") appears 1 time.\n",
      "Word 9 (\"keep\") appears 1 time.\n",
      "Word 10 (\"need\") appears 1 time.\n",
      "Word 11 (\"page\") appears 2 time.\n",
      "Word 12 (\"previous\") appears 1 time.\n",
      "Word 13 (\"process\") appears 1 time.\n",
      "Word 14 (\"refresh\") appears 1 time.\n",
      "Word 15 (\"right\") appears 1 time.\n",
      "Word 16 (\"test\") appears 1 time.\n",
      "Word 17 (\"ticket\") appears 1 time.\n",
      "Word 18 (\"tri\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview BOW for the nth submission in the data set\n",
    "\n",
    "slf_bow_first_sub = slf_bow_corpus[0]\n",
    "\n",
    "for i in range(len(slf_bow_first_sub)):\n",
    "    print(f'Word {slf_bow_first_sub[i][0]} (\"{slf_dictionary[slf_bow_first_sub[i][0]]}\") appears {slf_bow_first_sub[i][1]} time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# What does slf_bow_corpus actually look like?\n",
    "\n",
    "print(slf_bow_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selftext Topic Model (LDA)\n",
    "\n",
    "The bag-of-words model based on the selftext column is now ready to train a model. I have chosen to start with LDA. In addition to the corpus and the dictionary, the number of topics also needs to be provided. At this time I shall train the model for 10 topics. In the future, I may look to find a way to determine the optimum number of topics.\n",
    "\n",
    "I will run LDA using multiple CPU cores to parallelize and speed up the model training. \n",
    "\n",
    "some of the parameters involved are\n",
    "\n",
    "* **num_topics** is the number of topics to be extracted from the corpus.\n",
    "* **id2word** is a mapping from word id's to the actual words, in this case, our dictionary.\n",
    "* **workers** is the number of extra cores to use.\n",
    "* **passes** is the number of training passes through the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "slf_lda = gensim.models.LdaMulticore(slf_bow_corpus, num_topics=10, \n",
    "                                     id2word= slf_dictionary, passes=10, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Compute coherence score (used to judge how good a model is, higher is better)\n",
    "\n",
    "slf_lda_coherence_model = gensim.models.CoherenceModel(model=slf_lda, texts=subs_2['selftext'], \n",
    "                                                       dictionary=slf_dictionary, coherence='c_v')\n",
    "slf_lda_coherence = slf_lda_coherence_model.get_coherence()\n",
    "\n",
    "print(f'Coherence Score: {slf_lda_coherence:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"learn\" + 0.018*\"would\" + 0.017*\"program\" + 0.017*\"like\" + 0.016*\"know\" + 0.015*\"want\" + 0.013*\"start\" + 0.013*\"make\" + 0.012*\"help\" + 0.012*\"work\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.102*\"print\" + 0.042*\"input\" + 0.021*\"return\" + 0.020*\"els\" + 0.019*\"number\" + 0.018*\"enter\" + 0.018*\"import\" + 0.016*\"elif\" + 0.015*\"true\" + 0.015*\"code\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.017*\"post\" + 0.015*\"websit\" + 0.014*\"page\" + 0.014*\"find\" + 0.012*\"make\" + 0.012*\"link\" + 0.011*\"send\" + 0.011*\"email\" + 0.010*\"use\" + 0.010*\"user\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.055*\"file\" + 0.024*\"data\" + 0.017*\"script\" + 0.013*\"use\" + 0.013*\"write\" + 0.013*\"server\" + 0.012*\"request\" + 0.010*\"need\" + 0.009*\"json\" + 0.009*\"connect\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.057*\"imag\" + 0.037*\"pygam\" + 0.022*\"color\" + 0.018*\"import\" + 0.017*\"game\" + 0.017*\"screen\" + 0.016*\"card\" + 0.012*\"turtl\" + 0.011*\"draw\" + 0.011*\"rect\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.054*\"video\" + 0.042*\"text\" + 0.022*\"button\" + 0.021*\"root\" + 0.020*\"column\" + 0.020*\"grid\" + 0.019*\"label\" + 0.017*\"tkinter\" + 0.016*\"youtub\" + 0.015*\"share\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.109*\"self\" + 0.046*\"cython\" + 0.037*\"line\" + 0.037*\"build\" + 0.033*\"file\" + 0.030*\"copi\" + 0.026*\"includ\" + 0.024*\"win32\" + 0.021*\"user\" + 0.021*\"error\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.021*\"valu\" + 0.020*\"function\" + 0.020*\"list\" + 0.019*\"data\" + 0.014*\"name\" + 0.012*\"use\" + 0.011*\"class\" + 0.011*\"code\" + 0.010*\"number\" + 0.010*\"like\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.035*\"instal\" + 0.024*\"use\" + 0.021*\"work\" + 0.018*\"tri\" + 0.014*\"packag\" + 0.012*\"window\" + 0.012*\"error\" + 0.011*\"find\" + 0.011*\"version\" + 0.011*\"command\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.024*\"project\" + 0.022*\"use\" + 0.016*\"code\" + 0.014*\"github\" + 0.013*\"make\" + 0.012*\"librari\" + 0.010*\"work\" + 0.010*\"sourc\" + 0.010*\"tool\" + 0.010*\"develop\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualise the topics\n",
    "\n",
    "for idx, topic in slf_lda.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}')\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
